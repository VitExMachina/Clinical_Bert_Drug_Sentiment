{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PCA vs UMAP COMPARISON - METAL GPU VERSION\n",
        "# ============================================================\n",
        "# This notebook compares PCA and UMAP for dimensionality reduction\n",
        "# of ClinicalBERT embeddings before ANN classification\n",
        "# Optimized for Apple Silicon Macs with Metal GPU support\n",
        "\n",
        "# Install required packages\n",
        "%pip install transformers torch datasets scikit-learn umap-learn\n",
        "\n",
        "# Load basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import platform\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PCA vs UMAP Comparison - Metal GPU Version\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "print(f\"Processor: {platform.processor()}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================\n",
        "\n",
        "# Load TSV data\n",
        "drug_test_data = pd.read_csv(\"data/drugLibTest_raw.tsv\", sep='\\t')\n",
        "drug_train_data = pd.read_csv(\"data/drugLibTrain_raw.tsv\", sep='\\t')\n",
        "\n",
        "# Combine train and test data\n",
        "drug_data = pd.concat([drug_train_data, drug_test_data], ignore_index=True)\n",
        "\n",
        "# Drop missing values\n",
        "drug_data = drug_data.dropna()\n",
        "\n",
        "# Create text column\n",
        "drug_data = drug_data.dropna(subset=[\"urlDrugName\", \"rating\"])\n",
        "drug_data = drug_data[~drug_data[\"urlDrugName\"].str.lower().str.contains(\"unnamed\", na=False)]\n",
        "\n",
        "drug_data[\"text\"] = (\n",
        "    drug_data[[\"benefitsReview\", \"sideEffectsReview\", \"commentsReview\"]]\n",
        "    .fillna(\"\")\n",
        "    .agg(\" \".join, axis=1)\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "drug_data = drug_data[drug_data[\"text\"].str.len() > 10]\n",
        "\n",
        "# Map sentiment\n",
        "def map_sentiment(r):\n",
        "    if r <= 3:\n",
        "        return \"negative\"\n",
        "    elif r <= 6:\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return \"positive\"\n",
        "\n",
        "drug_data[\"sentiment\"] = drug_data[\"rating\"].apply(map_sentiment)\n",
        "\n",
        "# Map labels to integers\n",
        "label_order = [\"negative\", \"neutral\", \"positive\"]\n",
        "label2id = {l: i for i, l in enumerate(label_order)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "drug_data[\"label\"] = drug_data[\"sentiment\"].map(label2id)\n",
        "\n",
        "# Train/val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"sentiment\"\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    drug_data[[TEXT_COL, \"label\"]],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=drug_data[\"label\"]\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(drug_data)}\")\n",
        "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(drug_data[LABEL_COL].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DEVICE SETUP - METAL GPU\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score\n",
        "import umap\n",
        "\n",
        "# Detect device with priority: MPS (Metal) > CUDA > CPU\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"✓ Metal GPU (MPS) available - using Apple Silicon GPU\")\n",
        "    print(f\"Device: {device}\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"✓ CUDA GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Device: {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️  No GPU detected - using CPU\")\n",
        "    print(\"   For Apple Silicon Macs, ensure PyTorch with MPS support is installed\")\n",
        "    print(\"   Install: pip install torch torchvision torchaudio\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "print(f\"\\nPlatform: {platform.platform()}\")\n",
        "print(f\"Processor: {platform.processor()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1: CLINICALBERT EMBEDDING EXTRACTION\n",
        "# ============================================================\n",
        "\n",
        "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "MAX_LEN = 256\n",
        "\n",
        "print(\"Loading ClinicalBERT tokenizer and encoder...\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "enc_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "enc_model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Model hidden size: {enc_model.config.hidden_size}\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_for_enc(batch):\n",
        "    return tok(\n",
        "        batch[TEXT_COL], \n",
        "        truncation=True, \n",
        "        padding=\"max_length\", \n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "train_tok = train_ds.map(tokenize_for_enc, batched=True)\n",
        "val_tok = val_ds.map(tokenize_for_enc, batched=True)\n",
        "\n",
        "train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "val_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "print(f\"Train tokenized: {len(train_tok)} samples\")\n",
        "print(f\"Val tokenized: {len(val_tok)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract [CLS] token embeddings from ClinicalBERT\n",
        "def get_cls_embeddings(dataset, batch_size=16):\n",
        "    \"\"\"\n",
        "    Extract [CLS] token embeddings from ClinicalBERT.\n",
        "    Returns: X_emb (N, 768), y (N,)\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "    \n",
        "    print(f\"Extracting embeddings from {len(dataset)} samples...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "            \n",
        "            outputs = enc_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            cls_emb = outputs.last_hidden_state[:, 0, :]\n",
        "            \n",
        "            all_embeddings.append(cls_emb.cpu().numpy())\n",
        "            all_labels.append(labels)\n",
        "            \n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"  Processed {i + 1} batches...\")\n",
        "            \n",
        "            # MPS memory management\n",
        "            if device.type == \"mps\" and (i + 1) % 100 == 0:\n",
        "                torch.mps.empty_cache()\n",
        "    \n",
        "    X_emb = np.vstack(all_embeddings)\n",
        "    y = np.concatenate(all_labels)\n",
        "    \n",
        "    print(f\"Extracted embeddings shape: {X_emb.shape}\")\n",
        "    print(f\"Labels shape: {y.shape}\")\n",
        "    \n",
        "    return X_emb, y\n",
        "\n",
        "# Extract embeddings\n",
        "print(\"=\" * 60)\n",
        "print(\"Extracting ClinicalBERT Embeddings\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X_train_bert, y_train = get_cls_embeddings(train_tok, batch_size=16)\n",
        "X_val_bert, y_val = get_cls_embeddings(val_tok, batch_size=16)\n",
        "\n",
        "print(f\"\\nTrain embeddings: {X_train_bert.shape}, Labels: {y_train.shape}\")\n",
        "print(f\"Val embeddings: {X_val_bert.shape}, Labels: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 2: STANDARDIZE EMBEDDINGS\n",
        "# ============================================================\n",
        "# Both PCA and UMAP work better on standardized data\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Standardizing BERT Embeddings\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_bert)\n",
        "X_val_scaled = scaler.transform(X_val_bert)\n",
        "\n",
        "print(f\"Train scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"Val scaled shape: {X_val_scaled.shape}\")\n",
        "print(f\"Mean (should be ~0): {X_train_scaled.mean():.6f}\")\n",
        "print(f\"Std (should be ~1): {X_train_scaled.std():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# METHOD 1: PCA DIMENSIONALITY REDUCTION\n",
        "# ============================================================\n",
        "\n",
        "N_COMPONENTS = 50  # Target dimension for both methods\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"APPLYING PCA DIMENSIONALITY REDUCTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=N_COMPONENTS, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_val_pca = pca.transform(X_val_scaled)\n",
        "\n",
        "pca_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n✓ PCA completed in {pca_time:.2f} seconds\")\n",
        "print(f\"Original dimension: {X_train_bert.shape[1]}\")\n",
        "print(f\"PCA reduced dimension: {X_train_pca.shape[1]}\")\n",
        "print(f\"Variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "print(f\"Train PCA shape: {X_train_pca.shape}\")\n",
        "print(f\"Val PCA shape: {X_val_pca.shape}\")\n",
        "\n",
        "# Store for later comparison\n",
        "pca_results = {\n",
        "    'X_train': X_train_pca,\n",
        "    'X_val': X_val_pca,\n",
        "    'time': pca_time,\n",
        "    'variance_explained': pca.explained_variance_ratio_.sum()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# METHOD 2: UMAP DIMENSIONALITY REDUCTION\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"APPLYING UMAP DIMENSIONALITY REDUCTION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Note: UMAP is slower than PCA but may preserve better structure\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Apply UMAP\n",
        "# n_neighbors: balance between local vs global structure (smaller = faster, less global)\n",
        "# min_dist: controls how tightly points are packed (0.0 = tight, 1.0 = loose)\n",
        "umap_reducer = umap.UMAP(\n",
        "    n_components=N_COMPONENTS,\n",
        "    n_neighbors=15,          # Default: 15 (try 5-10 for faster, 20-30 for better quality)\n",
        "    min_dist=0.1,            # Default: 0.1 (try 0.0-0.5)\n",
        "    metric='euclidean',      # Distance metric\n",
        "    random_state=42,         # For reproducibility\n",
        "    n_jobs=-1                # Use all CPU cores\n",
        ")\n",
        "\n",
        "X_train_umap = umap_reducer.fit_transform(X_train_scaled)\n",
        "X_val_umap = umap_reducer.transform(X_val_scaled)\n",
        "\n",
        "umap_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n✓ UMAP completed in {umap_time:.2f} seconds ({umap_time/60:.2f} minutes)\")\n",
        "print(f\"Original dimension: {X_train_bert.shape[1]}\")\n",
        "print(f\"UMAP reduced dimension: {X_train_umap.shape[1]}\")\n",
        "print(f\"Train UMAP shape: {X_train_umap.shape}\")\n",
        "print(f\"Val UMAP shape: {X_val_umap.shape}\")\n",
        "\n",
        "# Store for later comparison\n",
        "umap_results = {\n",
        "    'X_train': X_train_umap,\n",
        "    'X_val': X_val_umap,\n",
        "    'time': umap_time\n",
        "}\n",
        "\n",
        "# Clear MPS cache if using Metal\n",
        "if device.type == \"mps\":\n",
        "    torch.mps.empty_cache()\n",
        "\n",
        "print(f\"\\n⏱️  Speed comparison:\")\n",
        "print(f\"   PCA:  {pca_time:.2f}s\")\n",
        "print(f\"   UMAP: {umap_time:.2f}s ({umap_time/pca_time:.1f}x slower)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZATION: 2D Projections\n",
        "# ============================================================\n",
        "# Visualize both methods in 2D for comparison\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Creating 2D Visualizations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create 2D versions for visualization\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "X_train_pca_2d = pca_2d.fit_transform(X_train_scaled)\n",
        "\n",
        "umap_2d = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42, n_jobs=-1)\n",
        "X_train_umap_2d = umap_2d.fit_transform(X_train_scaled)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# PCA 2D\n",
        "scatter1 = axes[0].scatter(\n",
        "    X_train_pca_2d[:, 0], X_train_pca_2d[:, 1],\n",
        "    c=y_train, cmap='viridis', alpha=0.6, s=20\n",
        ")\n",
        "axes[0].set_title('PCA 2D Projection\\n(Linear Variance)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('PC1')\n",
        "axes[0].set_ylabel('PC2')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Sentiment\\n(0=neg, 1=neu, 2=pos)')\n",
        "\n",
        "# UMAP 2D\n",
        "scatter2 = axes[1].scatter(\n",
        "    X_train_umap_2d[:, 0], X_train_umap_2d[:, 1],\n",
        "    c=y_train, cmap='viridis', alpha=0.6, s=20\n",
        ")\n",
        "axes[1].set_title('UMAP 2D Projection\\n(Non-linear Structure)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('UMAP1')\n",
        "axes[1].set_ylabel('UMAP2')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Sentiment\\n(0=neg, 1=neu, 2=pos)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualizations created\")\n",
        "print(\"\\nNote: Better clustering in UMAP suggests it may improve classification\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ANN MODEL DEFINITION\n",
        "# ============================================================\n",
        "\n",
        "class SentimentANN(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network for sentiment classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden=64, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_ann(X_train, X_val, y_train, y_val, method_name, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Train ANN on given features and return metrics.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training ANN on {method_name} features\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_train), \n",
        "        torch.LongTensor(y_train)\n",
        "    )\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.FloatTensor(X_val), \n",
        "        torch.LongTensor(y_val)\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SentimentANN(in_dim=X_train.shape[1], hidden=64, num_classes=3).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        all_val_preds = []\n",
        "        all_val_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_x)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                val_total += batch_y.size(0)\n",
        "                val_correct += (predicted == batch_y).sum().item()\n",
        "                \n",
        "                all_val_preds.extend(predicted.cpu().numpy())\n",
        "                all_val_labels.extend(batch_y.cpu().numpy())\n",
        "        \n",
        "        val_acc = val_correct / val_total\n",
        "        val_f1 = f1_score(all_val_labels, all_val_preds, average=\"macro\")\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        \n",
        "        # MPS memory management\n",
        "        if device.type == \"mps\" and (epoch + 1) % 3 == 0:\n",
        "            torch.mps.empty_cache()\n",
        "    \n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    \n",
        "    # Final evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(batch_y.cpu().numpy())\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    recall_macro = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    \n",
        "    # Per-class F1\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None, labels=[0,1,2])\n",
        "    \n",
        "    results = {\n",
        "        'method': method_name,\n",
        "        'accuracy': acc,\n",
        "        'macro_f1': f1_macro,\n",
        "        'macro_recall': recall_macro,\n",
        "        'f1_negative': f1_per_class[0],\n",
        "        'f1_neutral': f1_per_class[1],\n",
        "        'f1_positive': f1_per_class[2],\n",
        "        'training_time': training_time,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n✓ Training completed in {training_time:.2f} seconds\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")\n",
        "    print(f\"  Macro F1: {f1_macro:.4f}\")\n",
        "    print(f\"  Macro Recall: {recall_macro:.4f}\")\n",
        "    print(f\"  F1 per class - Negative: {f1_per_class[0]:.4f}, Neutral: {f1_per_class[1]:.4f}, Positive: {f1_per_class[2]:.4f}\")\n",
        "    \n",
        "    if device.type == \"mps\":\n",
        "        torch.mps.empty_cache()\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRAIN ANN ON PCA FEATURES\n",
        "# ============================================================\n",
        "\n",
        "pca_ann_results = train_ann(\n",
        "    X_train=pca_results['X_train'],\n",
        "    X_val=pca_results['X_val'],\n",
        "    y_train=y_train,\n",
        "    y_val=y_val,\n",
        "    method_name=\"PCA\",\n",
        "    num_epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRAIN ANN ON UMAP FEATURES\n",
        "# ============================================================\n",
        "\n",
        "umap_ann_results = train_ann(\n",
        "    X_train=umap_results['X_train'],\n",
        "    X_val=umap_results['X_val'],\n",
        "    y_train=y_train,\n",
        "    y_val=y_val,\n",
        "    method_name=\"UMAP\",\n",
        "    num_epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPREHENSIVE COMPARISON\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL COMPARISON: PCA vs UMAP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Method': ['PCA', 'UMAP'],\n",
        "    'Reduction Time (s)': [pca_results['time'], umap_results['time']],\n",
        "    'Reduction Time (min)': [pca_results['time']/60, umap_results['time']/60],\n",
        "    'Training Time (s)': [pca_ann_results['training_time'], umap_ann_results['training_time']],\n",
        "    'Total Time (s)': [\n",
        "        pca_results['time'] + pca_ann_results['training_time'],\n",
        "        umap_results['time'] + umap_ann_results['training_time']\n",
        "    ],\n",
        "    'Accuracy': [pca_ann_results['accuracy'], umap_ann_results['accuracy']],\n",
        "    'Macro F1': [pca_ann_results['macro_f1'], umap_ann_results['macro_f1']],\n",
        "    'Macro Recall': [pca_ann_results['macro_recall'], umap_ann_results['macro_recall']],\n",
        "    'F1 Negative': [pca_ann_results['f1_negative'], umap_ann_results['f1_negative']],\n",
        "    'F1 Neutral': [pca_ann_results['f1_neutral'], umap_ann_results['f1_neutral']],\n",
        "    'F1 Positive': [pca_ann_results['f1_positive'], umap_ann_results['f1_positive']],\n",
        "    'Variance Explained': [pca_results['variance_explained'], None]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Calculate improvements\n",
        "acc_improvement = umap_ann_results['accuracy'] - pca_ann_results['accuracy']\n",
        "f1_improvement = umap_ann_results['macro_f1'] - pca_ann_results['macro_f1']\n",
        "neutral_f1_improvement = umap_ann_results['f1_neutral'] - pca_ann_results['f1_neutral']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"IMPROVEMENT ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy improvement (UMAP vs PCA): {acc_improvement:+.4f} ({acc_improvement*100:+.2f}%)\")\n",
        "print(f\"Macro F1 improvement (UMAP vs PCA): {f1_improvement:+.4f} ({f1_improvement*100:+.2f}%)\")\n",
        "print(f\"Neutral F1 improvement (UMAP vs PCA): {neutral_f1_improvement:+.4f} ({neutral_f1_improvement*100:+.2f}%)\")\n",
        "print(f\"\\nTime cost (UMAP vs PCA): {umap_results['time']/pca_results['time']:.1f}x slower\")\n",
        "\n",
        "# Save results\n",
        "comparison_df.to_csv(\"pca_vs_umap_comparison.csv\", index=False)\n",
        "print(\"\\n✓ Saved comparison to: pca_vs_umap_comparison.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFUSION MATRICES COMPARISON\n",
        "# ============================================================\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title, ax):\n",
        "    \"\"\"Plot row-normalized confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
        "    \n",
        "    # Row-normalize to percentages\n",
        "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm_pct = np.where(row_sums > 0, (cm / row_sums) * 100.0, 0.0)\n",
        "    \n",
        "    im = ax.imshow(cm_pct, cmap=\"Blues\", aspect=\"auto\", vmin=0, vmax=100)\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"True\")\n",
        "    ax.set_xticks([0,1,2])\n",
        "    ax.set_xticklabels(label_order)\n",
        "    ax.set_yticks([0,1,2])\n",
        "    ax.set_yticklabels(label_order)\n",
        "    \n",
        "    # Add text annotations\n",
        "    for (i, j), val in np.ndenumerate(cm_pct):\n",
        "        ax.text(j, i, f\"{val:.1f}%\", ha=\"center\", va=\"center\", fontsize=10,\n",
        "                color=\"white\" if val > 50 else \"black\", weight=\"bold\")\n",
        "    \n",
        "    return im\n",
        "\n",
        "# Create side-by-side confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "im1 = plot_confusion_matrix(\n",
        "    pca_ann_results['labels'], \n",
        "    pca_ann_results['predictions'],\n",
        "    f\"PCA+ANN\\n(Accuracy: {pca_ann_results['accuracy']:.3f}, F1: {pca_ann_results['macro_f1']:.3f})\",\n",
        "    axes[0]\n",
        ")\n",
        "\n",
        "im2 = plot_confusion_matrix(\n",
        "    umap_ann_results['labels'], \n",
        "    umap_ann_results['predictions'],\n",
        "    f\"UMAP+ANN\\n(Accuracy: {umap_ann_results['accuracy']:.3f}, F1: {umap_ann_results['macro_f1']:.3f})\",\n",
        "    axes[1]\n",
        ")\n",
        "\n",
        "# Add colorbar\n",
        "plt.colorbar(im2, ax=axes, label=\"Row %\", fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Confusion matrices created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DETAILED CLASSIFICATION REPORTS\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLASSIFICATION REPORT: PCA+ANN\")\n",
        "print(\"=\" * 80)\n",
        "print(classification_report(\n",
        "    pca_ann_results['labels'], \n",
        "    pca_ann_results['predictions'],\n",
        "    target_names=label_order,\n",
        "    digits=3\n",
        "))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CLASSIFICATION REPORT: UMAP+ANN\")\n",
        "print(\"=\" * 80)\n",
        "print(classification_report(\n",
        "    umap_ann_results['labels'], \n",
        "    umap_ann_results['predictions'],\n",
        "    target_names=label_order,\n",
        "    digits=3\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RECOMMENDATION SUMMARY\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RECOMMENDATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "f1_improvement_pct = (umap_ann_results['macro_f1'] - pca_ann_results['macro_f1']) / pca_ann_results['macro_f1'] * 100\n",
        "time_cost = umap_results['time'] / pca_results['time']\n",
        "\n",
        "if f1_improvement_pct > 2.0:\n",
        "    recommendation = \"✅ RECOMMEND UMAP\"\n",
        "    reason = f\"UMAP improves macro F1 by {f1_improvement_pct:.1f}% (significant improvement)\"\n",
        "elif f1_improvement_pct > 0.5:\n",
        "    recommendation = \"⚠️  CONSIDER UMAP\"\n",
        "    reason = f\"UMAP improves macro F1 by {f1_improvement_pct:.1f}% (moderate improvement, but {time_cost:.1f}x slower)\"\n",
        "else:\n",
        "    recommendation = \"✅ RECOMMEND PCA\"\n",
        "    reason = f\"UMAP only improves by {f1_improvement_pct:.1f}% (not worth {time_cost:.1f}x time cost)\"\n",
        "\n",
        "print(f\"\\n{recommendation}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "print(f\"\\nKey Metrics:\")\n",
        "print(f\"  - PCA Accuracy: {pca_ann_results['accuracy']:.4f}\")\n",
        "print(f\"  - UMAP Accuracy: {umap_ann_results['accuracy']:.4f} ({umap_ann_results['accuracy']-pca_ann_results['accuracy']:+.4f})\")\n",
        "print(f\"  - PCA Macro F1: {pca_ann_results['macro_f1']:.4f}\")\n",
        "print(f\"  - UMAP Macro F1: {umap_ann_results['macro_f1']:.4f} ({umap_ann_results['macro_f1']-pca_ann_results['macro_f1']:+.4f})\")\n",
        "print(f\"  - PCA Neutral F1: {pca_ann_results['f1_neutral']:.4f}\")\n",
        "print(f\"  - UMAP Neutral F1: {umap_ann_results['f1_neutral']:.4f} ({umap_ann_results['f1_neutral']-pca_ann_results['f1_neutral']:+.4f})\")\n",
        "print(f\"  - PCA Time: {pca_results['time']:.2f}s\")\n",
        "print(f\"  - UMAP Time: {umap_results['time']:.2f}s ({time_cost:.1f}x slower)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
